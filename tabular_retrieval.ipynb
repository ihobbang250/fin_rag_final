{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6a564-b35e-4eb5-bba0-96bbe2616ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import voyageai\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaac2c0-c05d-4791-bf19-c3eaa35f80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_retrieval = ['TATQA', 'FinQA', 'ConvFinQA', 'MultiHiertt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7d352-560f-4031-bd07-823b427ef25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = voyageai.Client(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196cf70-24d9-4c05-82f9-9d6693432a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding\n",
    "model_name = \"voyage-3\"\n",
    "batch_size = 32\n",
    "delay = 0.1\n",
    "\n",
    "for task in tabular_retrieval:\n",
    "    for attr in [\"corpus\", \"queries\"]:\n",
    "        versions = [\"convert\", \"original\"] if attr == \"corpus\" else [\"original\"]\n",
    "\n",
    "        for version in versions:\n",
    "            file_path = f\"./data/{task}_{attr}_convert.csv\" if version == \"convert\" else f\"./data/{task}_{attr}.csv\"\n",
    "            data = pd.read_csv(file_path)\n",
    "            data = data.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "            embeddings = {}\n",
    "            for i in tqdm(range(0, len(data), batch_size), desc=f\"{attr} - {version} Batches\", leave=False):\n",
    "                batch = data[i:i + batch_size]\n",
    "                batch_ids = batch[\"_id\"].tolist()\n",
    "                batch_texts = batch[\"convert_text\"].tolist() if version == \"convert\" else batch[\"text\"].tolist()\n",
    "                emb_type = \"query\" if attr == \"queries\" else \"document\"\n",
    "                \n",
    "                try:\n",
    "                    result = vo.embed(batch_texts, model=model_name, input_type=emb_type).embeddings\n",
    "                    for _id, embedding in zip(batch_ids, result):\n",
    "                        embeddings[_id] = embedding\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error embedding batch starting at index {i} for task '{task}' - {e}\")\n",
    "                \n",
    "                # Sleep to respect rate limits\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # save\n",
    "            output_filename = f\"{task}_{attr}.json\" if version == \"original\" else f\"{task}_{attr}_convert.json\"\n",
    "            output_path = f\"./{model_name}/embed/{output_filename}\"\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660387b-a925-4289-890e-02b25c0a8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Score\n",
    "\n",
    "@torch.no_grad()\n",
    "def cos_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mm(\n",
    "        torch.nn.functional.normalize(a, p=2, dim=1),\n",
    "        torch.nn.functional.normalize(b, p=2, dim=1).transpose(0, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "for task in tabular_retrieval:\n",
    "    # Load query embeddings\n",
    "    with open(f\"./{model_name}/embed/{task}_queries.json\", \"r\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "        query_ids = list(loaded_data.keys())\n",
    "        queries_embeddings = torch.tensor([loaded_data[_id] for _id in query_ids])\n",
    "    \n",
    "    # Load corpus embeddings\n",
    "    with open(f\"./{model_name}/embed/{task}_corpus_convert.json\", \"r\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "        corpus_ids = list(loaded_data.keys())\n",
    "        corpus_embeddings = torch.tensor([loaded_data[_id] for _id in corpus_ids])\n",
    "    \n",
    "    similarity_matrix = cos_sim(queries_embeddings, corpus_embeddings)\n",
    "\n",
    "    top_k = 50 if task in ['FinQABench', 'FinanceBench'] else 500\n",
    "\n",
    "    top_matches = {}\n",
    "    \n",
    "    for i, query_id in enumerate(query_ids):\n",
    "        top_values, top_indices = torch.topk(similarity_matrix[i], top_k)\n",
    "        top_corpus_ids_scores = {corpus_ids[idx]: top_values[j].item() for j, idx in enumerate(top_indices)}\n",
    "        top_matches[query_id] = top_corpus_ids_scores\n",
    "    \n",
    "    output_path = f\"./{model_name}/{task}_convert.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(top_matches, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
