{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import financerag.tasks as tasks_module\n",
    "\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = ['FinDER', 'FinQABench', 'FinanceBench', 'TATQA', 'FinQA', 'ConvFinQA', 'MultiHiertt']\n",
    "tabular_retrieval = ['TATQA', 'FinQA', 'ConvFinQA', 'MultiHiertt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmm_normalize(scores, min_value, max_value):\n",
    "    \"\"\"Theoretical Min-Max Normalization with min/max calculated from metric.\"\"\"\n",
    "    normalized_scores = {}\n",
    "    for doc_id, score in scores.items():\n",
    "        normalized_scores[doc_id] = (score - min_value) / (max_value - min_value)\n",
    "    return normalized_scores\n",
    "\n",
    "def mm_normalize(scores):\n",
    "    \"\"\"Theoretical Min-Max Normalization with min/max calculated from data.\"\"\"\n",
    "    min_value = min(scores.values())\n",
    "    max_value = max(scores.values())\n",
    "    \n",
    "    if max_value == min_value:\n",
    "        return {doc_id: 0.0 for doc_id in scores}\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for doc_id, score in scores.items():\n",
    "        normalized_scores[doc_id] = (score - min_value) / (max_value - min_value)\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "def comb_sum_fusion_alpha(vector_result, lexical_result, alpha, normalize_type):\n",
    "    fused_results = {}\n",
    "    \n",
    "    # Iterate over each query in vector_result\n",
    "    for query_id in vector_result:\n",
    "        vector_scores = vector_result[query_id]\n",
    "        lexical_scores = lexical_result[query_id]\n",
    "\n",
    "        if normalize_type == \"tmm\":\n",
    "            normalized_vector_scores = tmm_normalize(vector_scores, min_value=-1, max_value=1)\n",
    "            normalized_lexical_scores = tmm_normalize(lexical_scores, min_value=0, max_value=1)\n",
    "        else:\n",
    "            normalized_vector_scores = mm_normalize(vector_scores)\n",
    "            normalized_lexical_scores = mm_normalize(lexical_scores)\n",
    "        \n",
    "        # Initialize a dictionary to store fused scores for the current query\n",
    "        fused_query_results = {}\n",
    "\n",
    "        # Sum normalized vector and lexical scores with alpha and (1 - alpha)\n",
    "        for doc_id, score in normalized_vector_scores.items():\n",
    "            fused_query_results[doc_id] = fused_query_results.get(doc_id, 0) + score * alpha\n",
    "\n",
    "        for doc_id, score in normalized_lexical_scores.items():\n",
    "            fused_query_results[doc_id] = fused_query_results.get(doc_id, 0) + score * (1 - alpha)\n",
    "\n",
    "        sorted_fused_query_results = dict(sorted(fused_query_results.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        fused_results[query_id] = sorted_fused_query_results\n",
    "\n",
    "    return fused_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A Hugging Face repository is provided. This will override the data_folder, prefix, and *_file arguments.\n",
      "A Hugging Face repository is provided. This will override the data_folder, prefix, and *_file arguments.\n",
      "A Hugging Face repository is provided. This will override the data_folder, prefix, and *_file arguments.\n",
      "A Hugging Face repository is provided. This will override the data_folder, prefix, and *_file arguments.\n"
     ]
    }
   ],
   "source": [
    "alpha_values = np.arange(0, 1.01, 0.01)\n",
    "results_list = []\n",
    "normalize_type = \"mm\"\n",
    "    \n",
    "for task_class in tabular_retrieval:\n",
    "    task_class_obj = getattr(tasks_module, task_class)\n",
    "    finder_task = task_class_obj()\n",
    "\n",
    "    lexical = \"BM25\"\n",
    "    \n",
    "    if task_class == \"MultiHiertt\":\n",
    "        dense = \"voyage-finance-2\"\n",
    "    elif task_class in [\"FinQABench\", \"FinanceBench\"]:\n",
    "        dense = \"financial-rag-matryoshka\"\n",
    "    elif task_class == \"FinDER\":\n",
    "        dense = \"stella_en_1.5B_v5\"\n",
    "    elif task_class in [\"TATQA\", \"FinQA\", \"ConvFinQA\"]:\n",
    "        dense = \"voyage-3\"\n",
    "\n",
    "    # Save Directory\n",
    "    output_dir = f'./hybrid_search'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    versions = [\"default\", \"convert\"] if task_class in tabular_retrieval else [\"default\"]\n",
    "\n",
    "    for version in versions:\n",
    "        # Set paths for the version\n",
    "        vector_result_path = f'./{dense}/{task_class}_{version}.json' if version != \"default\" else f'./{dense}/{task_class}.json'\n",
    "        lexical_result_path = f'./{lexical}/{task_class}_{version}.json' if version != \"default\" else f'./{lexical}/{task_class}.json'\n",
    "        \n",
    "        # Initialize to track the best NDCG@10 score and the corresponding alpha\n",
    "        best_ndcg_10 = -1\n",
    "        best_alpha = None\n",
    "\n",
    "        for alpha in alpha_values:\n",
    "            # Load results from both dense and lexical models\n",
    "            with open(vector_result_path, 'r', encoding='utf-8') as f:\n",
    "                vector_result = json.load(f)\n",
    "\n",
    "            with open(lexical_result_path, 'r', encoding='utf-8') as f:\n",
    "                lexical_result = json.load(f)\n",
    "\n",
    "            # Perform CombSUM fusion with the current alpha\n",
    "            fused_results = comb_sum_fusion_alpha(vector_result, lexical_result, alpha, normalize_type)\n",
    "\n",
    "            # Load and prepare qrels for evaluation\n",
    "            df = pd.read_csv(f'./eval/{task_class}_qrels.tsv', sep='\\t')\n",
    "            qrels_dict = df.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "            \n",
    "            eval_result = finder_task.evaluate(qrels_dict, fused_results, [1, 5, 10])\n",
    "            ndcg_10 = eval_result[0]['NDCG@10'] \n",
    "            \n",
    "            # Track the best alpha and NDCG@10 score\n",
    "            if ndcg_10 > best_ndcg_10:\n",
    "                best_ndcg_10 = ndcg_10\n",
    "                best_alpha = alpha\n",
    "                \n",
    "                # Save the best fused results for this version\n",
    "                output_file = f'{output_dir}/{task_class}_{version}_{normalize_type}_best_cc.json' if version != \"default\" else f'{output_dir}/{task_class}_{normalize_type}_best_cc.json'\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(fused_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Task': f\"{task_class}_{version}\" if version != \"default\" else task_class,\n",
    "            'Best Alpha': best_alpha,\n",
    "            'Best NDCG@10': best_ndcg_10\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert the results list to a DataFrame and save as a CSV file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv(f'{output_dir}/{normalize_type}_best_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9767247,
     "sourceId": 85594,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
